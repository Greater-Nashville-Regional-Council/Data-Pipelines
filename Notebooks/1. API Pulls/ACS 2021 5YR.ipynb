{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65aff7ad",
   "metadata": {},
   "source": [
    "###### Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97aedf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "from collections import deque\n",
    "from functools import reduce\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 150)\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b25189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../Functions and Dictionaries\") # Adds higher directory to python modules path\n",
    "import geodict\n",
    "GNRC = geodict.GNRC\n",
    "KY = geodict.KY\n",
    "censusplaces = geodict.censusplaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea503439",
   "metadata": {},
   "source": [
    "# American Community Survey 2017-2021 5 Year Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d9bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in API key\n",
    "with open('api_keys.pkl', 'rb') as keys_file:\n",
    "        keys_dict_2 = pickle.load(keys_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a63036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable containing Census API key\n",
    "api_key = keys_dict_2['CENSUS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdf3fa",
   "metadata": {},
   "source": [
    "## Read In Data Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7017a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataguide = pd.read_csv('../../Data Guides/DATA GUIDE ACS 2020_2021 5YR.csv', dtype = str)\n",
    "dataguide['ID'] = dataguide['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bff8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg1 = dataguide[dataguide['ID'].between(1, 46)]\n",
    "dg2 = dataguide[dataguide['ID'].between(47, 92)]\n",
    "dg3 = dataguide[dataguide['ID'].between(93, 138)]\n",
    "dg4 = dataguide[dataguide['ID'].between(139, 184)]\n",
    "dg5 = dataguide[dataguide['ID'].between(185, 230)]\n",
    "dg6 = dataguide[dataguide['ID'].between(231, 276)]\n",
    "dg7 = dataguide[dataguide['ID'].between(277, 322)]\n",
    "dg8 = dataguide[dataguide['ID'].between(323, 368)]\n",
    "dg9 = dataguide[dataguide['ID'].between(369, 414)]\n",
    "dg10 = dataguide[dataguide['ID'].between(415, 460)]\n",
    "dg11 = dataguide[dataguide['ID'].between(461, 506)]\n",
    "dg12 = dataguide[dataguide['ID'].between(507, 552)]\n",
    "dg13 = dataguide[dataguide['ID'].between(553, 598)]\n",
    "dg14 = dataguide[dataguide['ID'].between(599, 644)]\n",
    "dg15 = dataguide[dataguide['ID'].between(645, 690)]\n",
    "dg16 = dataguide[dataguide['ID'].between(691, 736)]\n",
    "dg17 = dataguide[dataguide['ID'].between(737, 782)]\n",
    "dg18 = dataguide[dataguide['ID'].between(783, 828)]\n",
    "dg19 = dataguide[dataguide['ID'].between(829, 874)]\n",
    "dg20 = dataguide[dataguide['ID'].between(875, 920)]\n",
    "dg21 = dataguide[dataguide['ID'].between(921, 966)]\n",
    "dg22 = dataguide[dataguide['ID'].between(967, 1012)]\n",
    "dg23 = dataguide[dataguide['ID'].between(1013, 1058)]\n",
    "dg24 = dataguide[dataguide['ID'].between(1059, 1104)]\n",
    "dg25 = dataguide[dataguide['ID'].between(1105, 1150)]\n",
    "dg26 = dataguide[dataguide['ID'].between(1151, 1196)]\n",
    "dg27 = dataguide[dataguide['ID'].between(1197, 1242)]\n",
    "dg28 = dataguide[dataguide['ID'].between(1243, 1287)]\n",
    "dg29 = dataguide[dataguide['ID'].between(1288, 1332)]\n",
    "dg30 = dataguide[dataguide['ID'].between(1333, 1377)]\n",
    "dg31 = dataguide[dataguide['ID'].between(1378, 1422)]\n",
    "dg32 = dataguide[dataguide['ID'].between(1423, 1467)]\n",
    "dg33 = dataguide[dataguide['ID'].between(1468, 1512)]\n",
    "dg34 = dataguide[dataguide['ID'].between(1513, 1557)]\n",
    "dg35 = dataguide[dataguide['ID'].between(1558, 1602)]\n",
    "dg36 = dataguide[dataguide['ID'].between(1603, 1647)]\n",
    "dg37 = dataguide[dataguide['ID'].between(1648, 1692)]\n",
    "dg38 = dataguide[dataguide['ID'].between(1693, 1737)]\n",
    "dg39 = dataguide[dataguide['ID'].between(1738, 1782)]\n",
    "dg40 = dataguide[dataguide['ID'].between(1783, 1827)]\n",
    "dg41 = dataguide[dataguide['ID'].between(1828, 1872)]\n",
    "dg42 = dataguide[dataguide['ID'].between(1873, 1917)]\n",
    "dg43 = dataguide[dataguide['ID'].between(1918, 1962)]\n",
    "dg44 = dataguide[dataguide['ID'].between(1963, 2007)]\n",
    "dg45 = dataguide[dataguide['ID'].between(2008, 2052)]\n",
    "dg46 = dataguide[dataguide['ID'].between(2053, 2097)]\n",
    "dg47 = dataguide[dataguide['ID'].between(2098, 2142)]\n",
    "dg48 = dataguide[dataguide['ID'].between(2143, 2187)]\n",
    "dg49 = dataguide[dataguide['ID'].between(2188, 2232)]\n",
    "dg50 = dataguide[dataguide['ID'].between(2233, 2277)]\n",
    "dg51 = dataguide[dataguide['ID'].between(2278, 2322)]\n",
    "dg52 = dataguide[dataguide['ID'].between(2323, 2367)]\n",
    "dg53 = dataguide[dataguide['ID'].between(2368, 2412)]\n",
    "dg54 = dataguide[dataguide['ID'].between(2413, 2457)]\n",
    "dg55 = dataguide[dataguide['ID'].between(2458, 2502)]\n",
    "dg56 = dataguide[dataguide['ID'].between(2503, 2547)]\n",
    "dg57 = dataguide[dataguide['ID'].between(2548, 2592)]\n",
    "dg58 = dataguide[dataguide['ID'].between(2593, 2637)]\n",
    "dg59 = dataguide[dataguide['ID'].between(2638, 2682)]\n",
    "dg60 = dataguide[dataguide['ID'].between(2683, 2727)]\n",
    "dg61 = dataguide[dataguide['ID'].between(2728, 2772)]\n",
    "dg62 = dataguide[dataguide['ID'].between(2773, 2817)]\n",
    "dg63 = dataguide[dataguide['ID'].between(2818, 2862)]\n",
    "dg64 = dataguide[dataguide['ID'].between(2863, 2907)]\n",
    "dg65 = dataguide[dataguide['ID'].between(2908, 2952)]\n",
    "dg66 = dataguide[dataguide['ID'].between(2953, 2997)]\n",
    "dg67 = dataguide[dataguide['ID'].between(2998, 3042)]\n",
    "dg68 = dataguide[dataguide['ID'].between(3043, 3086)]\n",
    "dg69 = dataguide[dataguide['ID'].between(3087, 3131)]\n",
    "dg70 = dataguide[dataguide['ID'].between(3132, 3176)]\n",
    "dg71 = dataguide[dataguide['ID'].between(3177, 3221)]\n",
    "dg72 = dataguide[dataguide['ID'].between(3222, 3266)]\n",
    "dg73 = dataguide[dataguide['ID'].between(3267, 3311)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005e2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [dg1, dg2, dg3, dg4, dg5, dg6, dg7, dg8, dg9, dg10, dg11, dg12, dg13, dg14, dg15, dg16, dg17, dg18, dg19, dg20, dg21, dg22, dg23, dg24, dg25, \n",
    "       dg26, dg27, dg28, dg29, dg30, dg31, dg32, dg33, dg34, dg35, dg36, dg37, dg38, dg39, dg40]\n",
    "dfs1 = [dg41, dg42, dg43, dg44, dg45, dg46, dg47, dg48, dg49,\n",
    "       dg50, dg51, dg52, dg53, dg54, dg55, dg56, dg57, dg58, dg59, dg60, dg61, dg62, dg63, dg64, dg65, dg66, dg67, dg68, dg69, dg70, dg71, dg72, dg73]\n",
    "dfs = [dg72, dg73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3972bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "droppers = ['ID', 'Data Point']\n",
    "for df in dfs:\n",
    "    df.drop(droppers, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7905b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppers = ['ID', 'Data Point']\n",
    "# for df in dfs1:\n",
    "#     df.drop(droppers, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7facb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url string and list parameters for column head and tail\n",
    "url_str= 'https://api.census.gov/data/2021/acs/acs5?key='+api_key\n",
    "head1 = 'NAME' \n",
    "head2 = 'GEO_ID'\n",
    "tail_cols1 = 'StateFIPS'\n",
    "tail_cols2 = 'GeoFIPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c259f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for df in dfs:\n",
    "    dataguide = df\n",
    "    for col_name, col_data in df.items():\n",
    "        var_list = list(dataguide['Variable']) #make variables list\n",
    "        var_list = deque(var_list)\n",
    "        var_list.appendleft(head2)\n",
    "        var_list.appendleft(head1)\n",
    "        col_list = list(dataguide['Column Name']) #make columns list\n",
    "        col_list.append(tail_cols1)\n",
    "        col_list.append(tail_cols2)\n",
    "        col_list = deque(col_list)\n",
    "        col_list.appendleft(head2)\n",
    "        col_list.appendleft(head1)\n",
    "        predicates= {} #call for all counties in state of TN\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"county:*\"\n",
    "        predicates[\"in\"]= \"state:47\"                                                             \n",
    "        data = requests.get(url_str, params= predicates)                                                                \n",
    "        col_names = col_list\n",
    "        df = pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        #df = df.loc[df['GeoFIPS'].isin(GNRC)] #filter for counties in the region\n",
    "        predicates= {} #call for all counties in state of KY\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"county:*\"\n",
    "        predicates[\"in\"]= \"state:21\"                                                             \n",
    "        data = requests.get(url_str, params= predicates)                                                                \n",
    "        col_names = col_list\n",
    "        kycos = pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        kycos = kycos.loc[kycos['GeoFIPS'].isin(KY)] #filter for counties of concern in KY\n",
    "        df = pd.concat([df, kycos], axis = 0)\n",
    "        predicates= {} #call for all places in TN\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"place:*\"\n",
    "        predicates[\"in\"]= \"state:47\"\n",
    "        data= requests.get(url_str, params= predicates)\n",
    "        col_names = col_list\n",
    "        places=pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        places=places.loc[places['GEO_ID'].isin(censusplaces)] #filter for places in the region\n",
    "        df = pd.concat([df, places], axis = 0)\n",
    "        predicates= {} #call for all places in KY\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"place:*\"\n",
    "        predicates[\"in\"]= \"state:21\"\n",
    "        data= requests.get(url_str, params= predicates)\n",
    "        col_names = col_list\n",
    "        places=pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        places=places.loc[places['GEO_ID'].isin(censusplaces)] #filter for places of concern in KY\n",
    "        df = pd.concat([df, places], axis = 0)\n",
    "        col_list.remove(tail_cols2) #adjust the column list for different formats of geos\n",
    "        predicates= {} #call for state of TN\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"state:47\"\n",
    "        data= requests.get(url_str, params= predicates)\n",
    "        col_names = col_list\n",
    "        state=pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        state['GeoFIPS'] = '0' #fill in the blank GeoFIPS column\n",
    "        df = pd.concat([df, state], axis = 0)\n",
    "        predicates= {} #call for US\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"us:*\"\n",
    "        data= requests.get(url_str, params= predicates)\n",
    "        col_names = col_list\n",
    "        national=pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        national['GeoFIPS'] = '0' #fill in the blank GeoFIPS column\n",
    "        df = pd.concat([df, national], axis = 0)\n",
    "        results.append(df)\n",
    "new_df = pd.concat(results, axis = 1)\n",
    "new_df = new_df.transpose().reset_index(drop = False).drop_duplicates()\n",
    "new_df.columns = new_df.iloc[0]\n",
    "new_df = new_df.set_index('NAME').transpose().drop(columns = ['StateFIPS', 'GeoFIPS']).reset_index(drop = True)\n",
    "new_df = new_df.rename_axis(None, axis = 1)\n",
    "print('Okay Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for df in dfs1:\n",
    "    dataguide = df\n",
    "    for col_name, col_data in df.items():\n",
    "        var_list = list(dataguide['Variable']) #make variables list\n",
    "        var_list = deque(var_list)\n",
    "        var_list.appendleft(head2)\n",
    "        var_list.appendleft(head1)\n",
    "        col_list = list(dataguide['Column Name']) #make columns list\n",
    "        col_list.append(tail_cols1)\n",
    "        col_list.append(tail_cols2)\n",
    "        col_list = deque(col_list)\n",
    "        col_list.appendleft(head2)\n",
    "        col_list.appendleft(head1)\n",
    "        predicates= {} #call for all counties in state of TN\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"county:*\"\n",
    "        predicates[\"in\"]= \"state:47\"                                                             \n",
    "        data = requests.get(url_str, params= predicates)                                                                \n",
    "        col_names = col_list\n",
    "        df = pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        #df = df.loc[df['GeoFIPS'].isin(GNRC)] #filter for counties in the region\n",
    "        predicates= {} #call for all counties in state of KY\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"county:*\"\n",
    "        predicates[\"in\"]= \"state:21\"                                                             \n",
    "        data = requests.get(url_str, params= predicates)                                                                \n",
    "        col_names = col_list\n",
    "        kycos = pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        kycos = kycos.loc[kycos['GeoFIPS'].isin(KY)] #filter for counties of concern in KY\n",
    "        df = pd.concat([df, kycos], axis = 0)\n",
    "        predicates= {} #call for all places in TN\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"place:*\"\n",
    "        predicates[\"in\"]= \"state:47\"\n",
    "        data= requests.get(url_str, params= predicates)\n",
    "        col_names = col_list\n",
    "        places=pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        places=places.loc[places['GEO_ID'].isin(censusplaces)] #filter for places in the region\n",
    "        df = pd.concat([df, places], axis = 0)\n",
    "        predicates= {} #call for all places in KY\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"place:*\"\n",
    "        predicates[\"in\"]= \"state:21\"\n",
    "        data= requests.get(url_str, params= predicates)\n",
    "        col_names = col_list\n",
    "        places=pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        places=places.loc[places['GEO_ID'].isin(censusplaces)] #filter for places of concern in KY\n",
    "        df = pd.concat([df, places], axis = 0)\n",
    "        col_list.remove(tail_cols2) #adjust the column list for different formats of geos\n",
    "        predicates= {} #call for state of TN\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"state:47\"\n",
    "        data= requests.get(url_str, params= predicates)\n",
    "        col_names = col_list\n",
    "        state=pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        state['GeoFIPS'] = '0' #fill in the blank GeoFIPS column\n",
    "        df = pd.concat([df, state], axis = 0)\n",
    "        predicates= {} #call for US\n",
    "        get_vars= var_list\n",
    "        predicates[\"get\"]= \",\". join(get_vars)\n",
    "        predicates[\"for\"]= \"us:*\"\n",
    "        data= requests.get(url_str, params= predicates)\n",
    "        col_names = col_list\n",
    "        national=pd.DataFrame(columns=col_names, data=data.json()[1:], dtype=str)\n",
    "        national['GeoFIPS'] = '0' #fill in the blank GeoFIPS column\n",
    "        df = pd.concat([df, national], axis = 0)\n",
    "        results.append(df)\n",
    "new_df1 = pd.concat(results, axis = 1)\n",
    "new_df1 = new_df1.transpose().reset_index(drop = False).drop_duplicates()\n",
    "new_df1.columns = new_df1.iloc[0]\n",
    "new_df1 = new_df1.set_index('NAME').transpose().drop(columns = ['StateFIPS', 'GeoFIPS']).reset_index(drop = True)\n",
    "new_df1 = new_df1.rename_axis(None, axis = 1)\n",
    "print('Okay Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42891c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index(['NAME', 'GEO_ID']).transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d858e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "numcols = list(data.columns)\n",
    "numcols\n",
    "data[numcols] = data[numcols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNRCCounties = [data[('Stewart County, Tennessee', '0500000US47161')],data[('Montgomery County, Tennessee', '0500000US47125')],\n",
    "                data[('Houston County, Tennessee', '0500000US47083')],data[('Humphreys County, Tennessee', '0500000US47085')],\n",
    "                data[('Dickson County, Tennessee', '0500000US47043')],data[('Cheatham County, Tennessee', '0500000US47021')],\n",
    "                data[('Robertson County, Tennessee', '0500000US47147')],data[('Sumner County, Tennessee', '0500000US47165')],\n",
    "                data[('Davidson County, Tennessee', '0500000US47037')],data[('Wilson County, Tennessee', '0500000US47189')],\n",
    "                data[('Trousdale County, Tennessee', '0500000US47169')],data[('Williamson County, Tennessee', '0500000US47187')],\n",
    "                data[('Rutherford County, Tennessee', '0500000US47149')]]\n",
    "data['GNRC'] = sum(GNRCCounties)\n",
    "GNRCCountiesAll = [data[('Stewart County, Tennessee', '0500000US47161')],data[('Montgomery County, Tennessee', '0500000US47125')],\n",
    "                   data[('Houston County, Tennessee', '0500000US47083')],data[('Humphreys County, Tennessee', '0500000US47085')],\n",
    "                   data[('Dickson County, Tennessee', '0500000US47043')],data[('Cheatham County, Tennessee', '0500000US47021')],\n",
    "                   data[('Robertson County, Tennessee', '0500000US47147')],data[('Sumner County, Tennessee', '0500000US47165')],\n",
    "                   data[('Davidson County, Tennessee', '0500000US47037')],data[('Wilson County, Tennessee', '0500000US47189')],\n",
    "                   data[('Trousdale County, Tennessee', '0500000US47169')],data[('Williamson County, Tennessee', '0500000US47187')],\n",
    "                   data[('Rutherford County, Tennessee', '0500000US47149')],data[('Maury County, Tennessee', '0500000US47119')]]\n",
    "data['GNRC Region'] = sum(GNRCCountiesAll)\n",
    "MPOCounties = [data[('Robertson County, Tennessee', '0500000US47147')],data[('Sumner County, Tennessee', '0500000US47165')],\n",
    "               data[('Davidson County, Tennessee', '0500000US47037')],data[('Wilson County, Tennessee', '0500000US47189')],\n",
    "               data[('Williamson County, Tennessee', '0500000US47187')],data[('Rutherford County, Tennessee', '0500000US47149')],\n",
    "               data[('Maury County, Tennessee', '0500000US47119')]]\n",
    "data['MPO'] = sum(MPOCounties)\n",
    "RuthInc = [data[('Eagleville city, Tennessee', '1600000US4722360')],data[('La Vergne city, Tennessee', '1600000US4741200')],\n",
    "           data[('Murfreesboro city, Tennessee', '1600000US4751560')],data[('Smyrna town, Tennessee', '1600000US4769420')]]\n",
    "data[('Rutherford Incorporated', 'None')] = sum(RuthInc)\n",
    "data[('Rutherford Unincorporated', 'None')] = data[('Rutherford County, Tennessee', '0500000US47149')] - data[('Rutherford Incorporated', 'None')]\n",
    "WilsonInc = [data[('Lebanon city, Tennessee', '1600000US4741520')],data[('Mount Juliet city, Tennessee', '1600000US4750780')],\n",
    "             data[('Watertown city, Tennessee', '1600000US4778320')]]\n",
    "data[('Wilson Incorporated', 'None')] = sum(WilsonInc)\n",
    "data[('Wilson Unincorporated', 'None')] = data[('Wilson County, Tennessee', '0500000US47189')] - data[('Wilson Incorporated', 'None')]\n",
    "CheathInc = [data[('Ashland City town, Tennessee', '1600000US4702180')],data[('Kingston Springs town, Tennessee', '1600000US4739660')],\n",
    "             data[('Pegram town, Tennessee', '1600000US4757480')],data[('Pleasant View city, Tennessee', '1600000US4759560')]]\n",
    "data[('Cheatham Incorporated', 'None')] = sum(CheathInc)\n",
    "data[('Cheatham Unincorporated', 'None')] = data[('Cheatham County, Tennessee', '0500000US47021')] - data[('Cheatham Incorporated', 'None')]\n",
    "DicksInc = [data[('Burns town, Tennessee', '1600000US4709880')],data[('Charlotte town, Tennessee', '1600000US4713080')],\n",
    "            data[('Dickson city, Tennessee', '1600000US4720620')],data[('Slayden town, Tennessee', '1600000US4769080')],\n",
    "            data[('Vanleer town, Tennessee', '1600000US4776860')],data[('White Bluff town, Tennessee', '1600000US4779980')]]\n",
    "data[('Dickson Incorporated', 'None')] = sum(DicksInc)\n",
    "data[('Dickson Unincorporated', 'None')] = data[('Dickson County, Tennessee', '0500000US47043')] - data[('Dickson Incorporated', 'None')]\n",
    "HumphInc = [data[('McEwen city, Tennessee', '1600000US4744840')],data[('New Johnsonville city, Tennessee', '1600000US4752820')],\n",
    "            data[('Waverly city, Tennessee', '1600000US4778560')]]\n",
    "data[('Humphreys Incorporated', 'None')] = sum(HumphInc)\n",
    "data[('Humphreys Unincorporated', 'None')] = data[('Humphreys County, Tennessee', '0500000US47085')] - data[('Humphreys Incorporated', 'None')]\n",
    "data[('Montgomery Incorporated', 'None')] = data[('Clarksville city, Tennessee', '1600000US4715160')]\n",
    "data[('Montgomery Unincorporated', 'None')] = data[('Montgomery County, Tennessee', '0500000US47125')] - data[('Montgomery Incorporated', 'None')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2533eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.transpose().reset_index()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09014ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddce31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_feather('../../Raw Data/ACS20215YR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../../Raw Data/ACS20215YR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3599b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
